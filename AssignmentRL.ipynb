{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AssignmentRL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8K8ZMFn14IN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba15fcf7-e65c-4fd0-b37e-8cf1442cadb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (3.22.4)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install cmake 'gym[atari]' scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the environment"
      ],
      "metadata": {
        "id": "0yyvqym9ULB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GVLfKvr4bsq",
        "outputId": "bf27e6fc-e20c-4271-91b8-831913111259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: |\u001b[43m \u001b[0m: :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.s = 301\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTFkE1WQ9hhI",
        "outputId": "af8cab0b-9ec9-4963-ad43-3942b22eb463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset() # reset environment to a new, random state\n",
        "env.render()\n",
        "\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywrO5gdk5VUB",
        "outputId": "09bf84d2-a142-4430-c3f8-3c7952ec6c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# State encoding and setting"
      ],
      "metadata": {
        "id": "uoddo4iYUP64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.encode(4, 0, 3, 2)\n",
        "state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU50TjrHCc9F",
        "outputId": "72a6db64-0235-4b53-beea-2c9a61d382b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "414"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.encode(4, 0, 3, 2) # (taxi row, taxi column, passenger index, destination index)\n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOA6q-o75fKG",
        "outputId": "d5007fa8-7e65-43f0-c5c4-c62e3809d6df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 414\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.P[416]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aijhnQ06E79A",
        "outputId": "dacdabe8-fafe-4fab-b080-3f8c383818c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 416, -1, False)],\n",
              " 1: [(1.0, 316, -1, False)],\n",
              " 2: [(1.0, 416, -1, False)],\n",
              " 3: [(1.0, 416, -1, False)],\n",
              " 4: [(1.0, 416, -10, False)],\n",
              " 5: [(1.0, 408, -1, False)]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "env.P[100]\n",
        "\n",
        "# env.s = 100\n",
        "# env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad_5g8jf9NHS",
        "outputId": "b2242e9c-a2d6-4bf4-c596-fca619f51ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 200, -1, False)],\n",
              " 1: [(1.0, 0, -1, False)],\n",
              " 2: [(1.0, 120, -1, False)],\n",
              " 3: [(1.0, 100, -1, False)],\n",
              " 4: [(1.0, 100, -10, False)],\n",
              " 5: [(1.0, 100, -10, False)]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Brute force approach\n"
      ],
      "metadata": {
        "id": "O40te3BtUfSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space.sample()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lduc5RVSIAqu",
        "outputId": "c9f7476a-996c-49ba-e0dd-a51675b574f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brute_force_approach(env,s): fuction that takes an environment and state this solving the environment without Reinforcement Learning\n",
        "     "
      ],
      "metadata": {
        "id": "LJD3X4OGAipw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Brute_force_approach(env,s):\n",
        "\n",
        "  env.s = 328  # set environment to illustration's state\n",
        "  \n",
        "  epochs = 0\n",
        "  penalties, rewards = 0, 0\n",
        "\n",
        "  frames = [] # for animation\n",
        "\n",
        "  done = False\n",
        "\n",
        "  \n",
        "  while not done:\n",
        "\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "\n",
        "    if reward == -10:\n",
        "        penalties += 1\n",
        "    \n",
        "    if reward > 0:\n",
        "      rewards += 1\n",
        "\n",
        "    # Put each rendered frame into dict for animation\n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "\n",
        "    epochs += 1\n",
        "    \n",
        "    \n",
        "  print(\"Timesteps taken: {}\".format(epochs))\n",
        "  print(\"Penalties incurred: {}\".format(penalties))\n"
      ],
      "metadata": {
        "id": "6NcyvYhbPIqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s=env.s = 328  # set environment to illustration's state\n",
        "Brute_force_approach(env,s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tN2T-xztmjm",
        "outputId": "a431a6c3-814e-4711-f624-82458c759c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timesteps taken: 936\n",
            "Penalties incurred: 288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(1)\n",
        "        \n"
      ],
      "metadata": {
        "id": "mfrQrZqePK4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "\n",
        "if random.uniform(0, 1) < 0.3:\n",
        "  print(\"ok\")"
      ],
      "metadata": {
        "id": "adWAWkH9ONcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Agent by Q_learning_approach, training(env,s): fuction that takes an environment and state this solving the environment by Reinforcement Learning the function takes an environment and the three parameters (alpha and gamma and epsilon)like this : training (env,a,g,e) , where alpha is the learning rate ,gamma is the discount factor\n",
        "     - Results : The Q-table is a matrix where we have a row for every state (500) and a column for every actions in 6 action that we have  (south,north ,east,west ,pickup,dropoff ) \n",
        "     "
      ],
      "metadata": {
        "id": "0gCUdDscAvXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training (env,a,g,e):\n",
        "\n",
        "  \n",
        "  %%time\n",
        "  \"\"\"Training the agent\"\"\"\n",
        "\n",
        "  import random\n",
        "  from IPython.display import clear_output\n",
        "  import numpy as np\n",
        "\n",
        "  # Initialize the q table\n",
        "  q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "  # Hyperparameters\n",
        "  alpha = a\n",
        "  gamma = g\n",
        "  epsilon = e\n",
        "\n",
        "  # For plotting metrics\n",
        "  all_epochs = []\n",
        "  all_penalties = []\n",
        "\n",
        "  for i in range(1, 100001):\n",
        "\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "\n",
        "      if random.uniform(0, 1) < epsilon:\n",
        "\n",
        "\n",
        "        action = env.action_space.sample() # Explore action space\n",
        "      else:\n",
        "\n",
        "        action = np.argmax(q_table[state]) # Exploit learned values\n",
        "\n",
        "      next_state, reward, done, info = env.step(action) \n",
        "        \n",
        "      old_value = q_table[state, action]\n",
        "      next_max = np.max(q_table[next_state])\n",
        "    \n",
        "      new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "      q_table[state, action] = new_value\n",
        "\n",
        "\n",
        "      if reward == -10:\n",
        "        penalties += 1\n",
        "\n",
        "      state = next_state\n",
        "      epochs += 1\n",
        "        \n",
        "      if i % 100 == 0:\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "  print(\"Training finished.\\n\")\n",
        "  return q_table\n"
      ],
      "metadata": {
        "id": "NyJPQ9DAwhru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "q_table=training (env,alpha,gamma,epsilon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP3BTQMCxvTl",
        "outputId": "cf80e1eb-1fb6-4f86-98a4-31f1b7de0ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.encode"
      ],
      "metadata": {
        "id": "YPOL15lAEsGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62c0f646-0d1a-4ef6-a39d-a1412582a26c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method TaxiEnv.encode of <gym.envs.toy_text.taxi.TaxiEnv object at 0x7f936ca66190>>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evalutation"
      ],
      "metadata": {
        "id": "IZ_rv1Z0U2Yx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the agent ,evaluation (q_table,env) : this function that takes a q_table and the environment , this fuction evaluates the performance of our agent. We don't need to explore actions any further, so now the next action is always selected using the best Q-value\n",
        "     -Results :this fuction return Average timesteps per episode and Average penalties per episode and Average rewards."
      ],
      "metadata": {
        "id": "aSqKarbmA6fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation (q_table,env):\n",
        "\n",
        "  \"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
        "\n",
        "  total_epochs, total_penalties ,total_rewards = 0, 0,0\n",
        "  episodes = 1000\n",
        "\n",
        "  for _ in range(episodes):\n",
        "\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "    total_rewards +=reward\n",
        "  avr_time_stampe=(total_epochs /episodes)\n",
        "  avr_penalties=(total_penalties / episodes)\n",
        "  avr_rewards=(total_rewards/episodes)\n",
        "\n",
        "  print(f\"Results after {episodes} episodes:\")\n",
        "  print(f\"Average timesteps per episode: {avr_time_stampe}\")\n",
        "  print(f\"Average penalties per episode: {avr_penalties}\")\n",
        "  return avr_time_stampe,avr_penalties,avr_rewards\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "WyFK9IpW1KU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avr_time_stampe,avr_penalties,avr_rewards=evaluation (q_table,env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVUYktwV2Kxx",
        "outputId": "35c9b00b-7d5f-454a-f3b6-5827a31d4e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.092\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TUNNING"
      ],
      "metadata": {
        "id": "z4SCyE2l3Y-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tunning :The values of `alpha`, `gamma`, and `epsilon` , all three should decrease over time because as the agent continues to learn, give to parameters (alpha , gamma, epsilon)different values from (0.1 :0.9) and try which parameters will affect in the environment "
      ],
      "metadata": {
        "id": "L97t7D2SBRxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\"\"\"Training the agent\"\"\"\n",
        "\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the q table\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.7\n",
        "gamma = 0.7\n",
        "epsilon = 0.6\n",
        "\n",
        "# For plotting metrics\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "count=0\n",
        "for i in range(1, 100001):\n",
        "  \n",
        "  state = env.reset()\n",
        "\n",
        "    \n",
        "  epochs, penalties, reward, = 0, 0, 0\n",
        "  done = False\n",
        "    \n",
        "  while not done:\n",
        "      if random.uniform(0, 1) < epsilon:\n",
        "          action = env.action_space.sample() # Explore action space\n",
        "      else:\n",
        "          action = np.argmax(q_table[state]) # Exploit learned values\n",
        "\n",
        "      next_state, reward, done, info = env.step(action) \n",
        "        \n",
        "      old_value = q_table[state, action]\n",
        "      next_max = np.max(q_table[next_state])\n",
        "    \n",
        "      new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "      q_table[state, action] = new_value\n",
        "\n",
        "      if reward == -10:\n",
        "          penalties += 1\n",
        "\n",
        "      state = next_state\n",
        "      epochs += 1\n",
        "        \n",
        "      if i % 100 == 0:\n",
        "        alpha =alpha*0.9\n",
        "        gamma =gamma*0.9\n",
        "        epsilon =epsilon*0.9\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "      count+=1\n",
        "print(\"Training finished.\\n\")"
      ],
      "metadata": {
        "id": "oElQ3ZdE3lGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREAT A GRID SEARCH ,as the training take a number of hours to train 10 values for every parameter , for this reason ,I take 3 values only for every parameter "
      ],
      "metadata": {
        "id": "pQmxIfSTBV-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "alpha_list=[0.2,0.6,0.8]\n",
        "gamma_list=[0.2,0.6,0.8]\n",
        "epsilon_list=[0.2,0.6,0.8]\n",
        "lis=[alpha_list,gamma_list,epsilon_list]\n",
        "grid_search=list(itertools.product(*lis))\n"
      ],
      "metadata": {
        "id": "yZnB6clkXdQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Hyper Parameter Tunning: making a grid search and try which parameters will give us highest evaluation by using this formula (rewords/(avr_penalties+avr_time_stampe)) \n",
        "to choose parameters which enable us to get the maximum reward as fast as possible. "
      ],
      "metadata": {
        "id": "df69BNdbY3jR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the hyper Parameter Tunning takes more than 3 hours to train the Agent with 3 values for every parameter"
      ],
      "metadata": {
        "id": "yObvo2U1DdNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_Evaluations=[]\n",
        "for i in range(0,len(grid_search)):\n",
        "\n",
        "  l=grid_search[i]\n",
        "  alpha=l[0]\n",
        "  gamma=l[1]\n",
        "  epsilon=l[2]\n",
        "  q_table =training(env,alpha,gamma,epsilon)\n",
        "  avr_time_stampe,avr_penalties,rewords=evaluation(q_table,env)\n",
        "  eva=(rewords/(avr_penalties+avr_time_stampe))\n",
        "  all_Evaluations.append(eva)\n"
      ],
      "metadata": {
        "id": "ZmMINBjGpxD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef29804d-212d-46fb-ad74-9c8829b954f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "Results after 1000 episodes:\n",
            "Average timesteps per episode: 13.234\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_Evaluations"
      ],
      "metadata": {
        "id": "aWs49QvCw0QE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db154ada-30be-4a87-93fe-52b5bb7803e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5124016938898972, 1.511258878645912]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_evaluation= max(all_Evaluations)\n",
        "high_parameter = all_Evaluations.index(max_evaluation)\n",
        "grid_search[high_parameter]"
      ],
      "metadata": {
        "id": "39Akm5G0xZ2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26c3195e-e222-4751-f5e9-e7df0f72f4dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2, 0.2, 0.2)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}